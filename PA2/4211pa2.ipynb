{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Data Split\n",
    "Use train_dataset and eval_dataset as train / test sets\n",
    "\n",
    "'''\n",
    "from torchvision.datasets import EMNIST\n",
    "from torch.utils.data import ConcatDataset, Subset\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "import numpy as np\n",
    "\n",
    "# the library imported by myself\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam, SGD # just choose which to use\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# For convenience, show image at index in dataset\n",
    "def show_image(dataset, index):\n",
    "  \n",
    "  plt.imshow(dataset[index][0][0], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "def get_datasets(split='balanced', save=False):\n",
    "  download_folder = './data'\n",
    "\n",
    "  transform = Compose([ToTensor()])\n",
    "\n",
    "  dataset = ConcatDataset([EMNIST(root=download_folder, split=split, download=True, train=False, transform=transform),\n",
    "                           EMNIST(root=download_folder, split=split, download=True, train=True, transform=transform)])\n",
    "\n",
    "  # save already = false, the program will not go into the below part, I just leave it.\n",
    "  if save:\n",
    "    random_seed = 4211 # do not change\n",
    "    n_samples = len(dataset)\n",
    "    eval_size = 0.2\n",
    "    indices = list(range(n_samples))\n",
    "    split = int(np.floor(eval_size * n_samples))\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_indices, eval_indices = indices[split:], indices[:split]\n",
    "\n",
    "    # cut to half\n",
    "    train_indices = train_indices[:len(train_indices)//2]\n",
    "    eval_indices = eval_indices[:len(eval_indices)//2]\n",
    "\n",
    "    np.savez('train_test_split.npz', train=train_indices, test=eval_indices)\n",
    "\n",
    "  # This is what really happen\n",
    "  # load train test split indices\n",
    "  else:\n",
    "    with np.load('./train_test_split.npz') as f:\n",
    "      train_indices = f['train']\n",
    "      eval_indices = f['test']\n",
    "\n",
    "  train_dataset = Subset(dataset, indices=train_indices)\n",
    "  eval_dataset = Subset(dataset, indices=eval_indices)\n",
    "\n",
    "  return train_dataset, eval_dataset\n",
    "\n",
    "# TODO\n",
    "# 1. build your own CNN classifier with the given structure. DO NOT COPY OR USE ANY TRICK\n",
    "# 2. load pretrained encoder from 'pretrained_encoder.pt' and build a CNN classifier on top of the encoder\n",
    "# 3. load pretrained encoder from 'pretrained_encoder.pt' and build a Convolutional Autoencoder on top of the encoder (just need to implement decoder)\n",
    "# *** Note that all the above tasks include implementation, training, analyzing, and reporting\n",
    "\n",
    "# example main code\n",
    "# each img has size (1, 28, 28) and each label is in {0, ..., 46}, a total of 47 classes\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "# For the task 3.2.1 : constructing the CNN model required\n",
    "class Cnn_Scratch_Classifier(nn.Module):\n",
    "  def __init__(self, n_hidden):\n",
    "    super(Cnn_Scratch_Classifier, self).__init__()\n",
    "\n",
    "    # in_data size: (batch_size, 1, 28, 28)\n",
    "    self.cnn_layers = nn.Sequential(\n",
    "      # conv1_out size: (batch_size, 4, 26, 26)\n",
    "      nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding=0),\n",
    "      nn.ReLU(),\n",
    "      # conv2_out size: (batch_size, 8, 12, 12)\n",
    "      nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=2, padding=0),\n",
    "      nn.ReLU(),\n",
    "      # conv3_out size: (batch_size, 16, 5, 5)\n",
    "      nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=0),\n",
    "      nn.ReLU(),\n",
    "      # MaxPool2d_out size: (batch_size, 16, 3, 3)\n",
    "      nn.MaxPool2d(kernel_size=3, stride=1, padding=0),\n",
    "      # conv4_out size: (batch_size, 32, 1, 1)\n",
    "      nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0),\n",
    "      # As required, just after the last convolution layer, logistic function should be used\n",
    "      nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "    # linear layers transforms flattened image features into logits before the softmax layer\n",
    "    self.linear = nn.Sequential(\n",
    "      nn.Linear(32, n_hidden),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(n_hidden, 47) # there are 47 predicted_labels\n",
    "    )\n",
    "\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "    self.loss_fn = nn.CrossEntropyLoss(reduction='sum') # will be divided by batch size\n",
    "\n",
    "  def forward(self, in_data):\n",
    "    img_features = self.cnn_layers(in_data).view(in_data.size(0), 32) # in_data.size(0) == batch_size\n",
    "    logits = self.linear(img_features)\n",
    "    return logits\n",
    "\n",
    "  def loss(self, logits, labels):\n",
    "    #preds = self.softmax(logits) # size (batch_size, 10)\n",
    "    return self.loss_fn(logits, labels) / logits.size(0) # divided by batch_size\n",
    "\n",
    "\n",
    "  def top1_accuracy(self, logits, labels):\n",
    "    # get argmax of logits along dim=1 (this is equivalent to argmax of predicted probabilites)\n",
    "    predicted_labels = torch.argmax(logits, dim=1, keepdim=False)\n",
    "    n_corrects = float(predicted_labels.eq(labels).sum(0)) # sum up all the correct predictions\n",
    "    return int(n_corrects / logits.size(0) * 100) # in percentage\n",
    "\n",
    "  def top3_accuracy(self, logits, labels):\n",
    "    n_corrects = 0\n",
    "    # copy logits for implementation without chage original value\n",
    "    logits_copy = logits.clone().detach()\n",
    "    # get argmax of logits along dim=1 (this is equivalent to argmax of predicted probabilites)\n",
    "    max1 = torch.argmax(logits_copy, dim=1)\n",
    "    n_corrects = n_corrects + max1.eq(labels).sum(0)\n",
    "    for i in range(max1.shape[0]):\n",
    "        logits_copy[i][max1[i]] = -99999\n",
    "\n",
    "    max2 = torch.argmax(logits_copy, dim=1)\n",
    "    n_corrects = n_corrects + max2.eq(labels).sum(0)\n",
    "    for i in range(max2.shape[0]):\n",
    "        logits_copy[i][max2[i]] = -99999\n",
    "\n",
    "    max3 = torch.argmax(logits_copy, dim=1)\n",
    "    n_corrects = float(n_corrects + max3.eq(labels).sum(0))\n",
    "    for i in range(max3.shape[0]):\n",
    "        logits_copy[i][max3[i]] = -99999\n",
    "\n",
    "    return int((n_corrects / logits.size(0)) * 100) # in percentage\n",
    "\n",
    "######################################################################################################################################\n",
    "\n",
    "# For the task 3.2.2: Learning from Pretrained Encoder Weights\n",
    "\n",
    "\n",
    "# just load the pt file, then do the same linear operation as Cnn_Scratch_Classifier\n",
    "\n",
    "class Cnn_Pretrained_Classifier(nn.Module):\n",
    " def __init__(self, n_hidden):\n",
    "   \n",
    "   super(Cnn_Pretrained_Classifier, self).__init__()\n",
    "   \n",
    "   self.cnn_layers = torch.load('pretrained_encoder.pt', map_location = 'cpu')['model']\n",
    "\n",
    "   # linear layers transforms flattened image features into logits before the softmax layer\n",
    "   self.linear = nn.Sequential(\n",
    "     nn.Linear(32, n_hidden),\n",
    "     nn.ReLU(),\n",
    "     nn.Linear(n_hidden, 47) # there are 47 predicted_labels\n",
    "   )\n",
    "\n",
    "   self.softmax = nn.Softmax(dim=1)\n",
    "   self.loss_fn = nn.CrossEntropyLoss(reduction='sum') # will be divided by batch size\n",
    "\n",
    " def forward(self, in_data):\n",
    "    img_features = self.cnn_layers(in_data).view(in_data.size(0), 32) # in_data.size(0) == batch_size\n",
    "    logits = self.linear(img_features)\n",
    "    return logits\n",
    "\n",
    " def loss(self, logits, labels):\n",
    "    #preds = self.softmax(logits) # size (batch_size, 10)\n",
    "    return self.loss_fn(logits, labels) / logits.size(0) # divided by batch_size\n",
    "\n",
    "\n",
    " def top1_accuracy(self, logits, labels):\n",
    "   # get argmax of logits along dim=1 (this is equivalent to argmax of predicted probabilites)\n",
    "   predicted_labels = torch.argmax(logits, dim=1, keepdim=False) # size (batch_size,)\n",
    "   n_corrects = float(predicted_labels.eq(labels).sum(0)) # sum up all the correct predictions\n",
    "   return int(n_corrects / logits.size(0) * 100) # in percentage\n",
    "\n",
    "\n",
    " def top3_accuracy(self, logits, labels):\n",
    "    n_corrects = 0\n",
    "    # copy logits for implementation without chage original value\n",
    "    logits_copy = logits.clone().detach()\n",
    "    # get argmax of logits along dim=1 (this is equivalent to argmax of predicted probabilites)\n",
    "    max1 = torch.argmax(logits_copy, dim=1)\n",
    "    n_corrects = n_corrects + max1.eq(labels).sum(0)\n",
    "    for i in range(max1.shape[0]):\n",
    "        logits_copy[i][max1[i]] = -99999\n",
    "\n",
    "    max2 = torch.argmax(logits_copy, dim=1)\n",
    "    n_corrects = n_corrects + max2.eq(labels).sum(0)\n",
    "    for i in range(max2.shape[0]):\n",
    "        logits_copy[i][max2[i]] = -99999\n",
    "\n",
    "    max3 = torch.argmax(logits_copy, dim=1)\n",
    "    n_corrects = float(n_corrects + max3.eq(labels).sum(0))\n",
    "    for i in range(max3.shape[0]):\n",
    "        logits_copy[i][max3[i]] = -99999\n",
    "\n",
    "    return int(n_corrects / logits.size(0) * 100 )# in percentage\n",
    "\n",
    "######################################################################################################################################3\n",
    "\n",
    "# Train function\n",
    "# Notice that the eval here is not the test set, but the validation set.\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def train(model, loaders, optimizer, writer, n_epochs, ckpt_path, device='cpu'):\n",
    "  def run_epoch(train_or_eval):\n",
    "    epoch_loss = 0.\n",
    "    epoch_acc_1 = 0.\n",
    "    epoch_acc_3 = 0.\n",
    "\n",
    "    for i, batch in enumerate(loaders[train_or_eval], 1):\n",
    "      in_data, labels = batch\n",
    "      in_data, labels = in_data.to(device), labels.to(device)\n",
    "\n",
    "      if train_or_eval == 'train':\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "      logits = model(in_data)\n",
    "      \n",
    "      batch_loss = model.loss(logits, labels)\n",
    "      batch_acc_1 = model.top1_accuracy(logits, labels)\n",
    "      batch_acc_3 = model.top3_accuracy(logits, labels)\n",
    "\n",
    "      epoch_loss += batch_loss\n",
    "      epoch_acc_1 += batch_acc_1\n",
    "      epoch_acc_3 += batch_acc_3\n",
    "\n",
    "      if train_or_eval == 'train':\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss /= i\n",
    "    epoch_acc_1 /= i\n",
    "    epoch_acc_3 /= i\n",
    "    print('loss: %s    top1 acc: %s     top3 acc: %s' %(epoch_loss,epoch_acc_1,epoch_acc_3))\n",
    "    print()\n",
    "\n",
    "    losses[train_or_eval] = epoch_loss\n",
    "    accs_1[train_or_eval] = epoch_acc_1\n",
    "    accs_3[train_or_eval] = epoch_acc_3\n",
    "\n",
    "    if writer is None:\n",
    "      print('epoch %d %s loss %.4f acc %.4f' % (epoch, train_or_eval, epoch_loss, epoch_acc))\n",
    "    elif train_or_eval == 'eval':\n",
    "      writer.add_scalars('%s_loss' % model.__class__.__name__,\n",
    "                         tag_scalar_dict={'train': losses['train'],\n",
    "                                          'eval': losses['eval']},\n",
    "                         global_step=epoch)\n",
    "\n",
    "      writer.add_scalars('%s_top1_accuracy' % model.__class__.__name__,  \n",
    "                         tag_scalar_dict={'train': accs_1['train'],\n",
    "                                          'eval': accs_1['eval']},\n",
    "                         global_step=epoch)\n",
    "\n",
    "      writer.add_scalars('%s_top3_accuracy' % model.__class__.__name__,  # CnnClassifier or FcClassifier\n",
    "                         tag_scalar_dict={'train': accs_3['train'],\n",
    "                                          'eval': accs_3['eval']},\n",
    "                         global_step=epoch)\n",
    "\n",
    "      # For instructional purpose, add images here, just the last in_data\n",
    "      #if epoch % 10 == 0:\n",
    "      #  if len(in_data.size()) == 2: # when it is flattened, reshape it\n",
    "      #    in_data = in_data.view(-1, 1, 28, 28)\n",
    "\n",
    "      #  img_grid = make_grid(in_data.to('cpu'))\n",
    "      #  writer.add_image('%s/eval_input' % model.__class__.__name__, img_grid, epoch)\n",
    "\n",
    "  # main statements\n",
    "  losses = dict()\n",
    "  accs_1 = dict()\n",
    "  accs_3 = dict()\n",
    "\n",
    "  for epoch in range(1, n_epochs+1):\n",
    "    print('Epoch %s :' %epoch)\n",
    "    print('Train:')\n",
    "    run_epoch('train')\n",
    "    print('Eval:')\n",
    "    run_epoch('eval')\n",
    "\n",
    "    # For instructional purpose, show how to save checkpoints\n",
    "    if ckpt_path is not None:\n",
    "      torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'losses': losses,\n",
    "        'accs_1': accs_1,\n",
    "        'accs_3': accs_3\n",
    "      }, '%s/%d.pt' % (ckpt_path, epoch))\n",
    "\n",
    "######################################################################################################################################\n",
    "\n",
    "# this train function is for training the CNN with the optimal parameter set, which do not have holdout\n",
    "def train_for_test(model, loaders, optimizer, writer, n_epochs, ckpt_path, device='cpu'):\n",
    "  def run_epoch(train_or_eval):\n",
    "    epoch_loss = 0.\n",
    "    epoch_acc_1 = 0.\n",
    "    epoch_acc_3 = 0.\n",
    "\n",
    "    for i, batch in enumerate(loaders[train_or_eval], 1):\n",
    "      in_data, labels = batch\n",
    "      in_data, labels = in_data.to(device), labels.to(device)\n",
    "\n",
    "      if train_or_eval == 'train':\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "      logits = model(in_data)\n",
    "      \n",
    "      batch_loss = model.loss(logits, labels)\n",
    "      batch_acc_1 = model.top1_accuracy(logits, labels)\n",
    "      batch_acc_3 = model.top3_accuracy(logits, labels)\n",
    "\n",
    "      epoch_loss += batch_loss\n",
    "      epoch_acc_1 += batch_acc_1\n",
    "      epoch_acc_3 += batch_acc_3\n",
    "\n",
    "      if train_or_eval == 'train':\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss /= i\n",
    "    epoch_acc_1 /= i\n",
    "    epoch_acc_3 /= i\n",
    "    print('top1 acc: %s     top3 acc: %s' %(epoch_acc_1,epoch_acc_3))\n",
    "    print()\n",
    "\n",
    "    losses[train_or_eval] = epoch_loss\n",
    "    accs_1[train_or_eval] = epoch_acc_1\n",
    "    accs_3[train_or_eval] = epoch_acc_3\n",
    "\n",
    "    if writer is None:\n",
    "      print('epoch %d %s loss %.4f acc %.4f' % (epoch, train_or_eval, epoch_loss, epoch_acc))\n",
    "    elif train_or_eval == 'eval':\n",
    "      writer.add_scalars('%s_loss' % model.__class__.__name__,\n",
    "                         tag_scalar_dict={'train': losses['train'],\n",
    "                                          'eval': losses['eval']},\n",
    "                         global_step=epoch)\n",
    "\n",
    "      writer.add_scalars('%s_top1_accuracy' % model.__class__.__name__,  \n",
    "                         tag_scalar_dict={'train': accs_1['train'],\n",
    "                                          'eval': accs_1['eval']},\n",
    "                         global_step=epoch)\n",
    "\n",
    "      writer.add_scalars('%s_top3_accuracy' % model.__class__.__name__,  # CnnClassifier or FcClassifier\n",
    "                         tag_scalar_dict={'train': accs_3['train'],\n",
    "                                          'eval': accs_3['eval']},\n",
    "                         global_step=epoch)\n",
    "\n",
    "      # For instructional purpose, add images here, just the last in_data\n",
    "      #if epoch % 10 == 0:\n",
    "      #  if len(in_data.size()) == 2: # when it is flattened, reshape it\n",
    "      #    in_data = in_data.view(-1, 1, 28, 28)\n",
    "\n",
    "      #  img_grid = make_grid(in_data.to('cpu'))\n",
    "      #  writer.add_image('%s/eval_input' % model.__class__.__name__, img_grid, epoch)\n",
    "\n",
    "  # main statements\n",
    "  losses = dict()\n",
    "  accs_1 = dict()\n",
    "  accs_3 = dict()\n",
    "\n",
    "  for epoch in range(1, n_epochs+1):\n",
    "    print('Epoch %s :' %epoch)\n",
    "    print('Train:')\n",
    "    run_epoch('train')\n",
    "    print('Test:')\n",
    "    run_epoch('eval')\n",
    "\n",
    "    # For instructional purpose, show how to save checkpoints\n",
    "    if ckpt_path is not None:\n",
    "      torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'losses': losses,\n",
    "        'accs_1': accs_1,\n",
    "        'accs_3': accs_3\n",
    "      }, '%s/%d.pt' % (ckpt_path, epoch))\n",
    "\n",
    "\n",
    "###########################################################################################################333\n",
    "\n",
    "\n",
    "class Cnn_Reconstructor(nn.Module):\n",
    " def __init__(self):\n",
    "   super(Cnn_Reconstructor, self).__init__()\n",
    "\n",
    "   self.cnn_layers = torch.load('pretrained_encoder.pt', map_location = 'cpu')['model']\n",
    "   \n",
    "   # linear layers transforms flattened image features into logits before the softmax layer\n",
    "   self.decoder = nn.Sequential(\n",
    "     \n",
    "     nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=0),\n",
    "     nn.ReLU(),\n",
    "\n",
    "     nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=0),\n",
    "     nn.ReLU(),\n",
    "\n",
    "     nn.ConvTranspose2d(in_channels=8, out_channels=8, kernel_size=3, stride=2, padding=0),\n",
    "     nn.ReLU(),\n",
    "\n",
    "     nn.ConvTranspose2d(in_channels=8, out_channels=4, kernel_size=3, stride=1, padding=0),\n",
    "     nn.ReLU(),\n",
    "\n",
    "     nn.ConvTranspose2d(in_channels=4, out_channels=1, kernel_size=4, stride=2, padding=0),\n",
    "\n",
    "     # As required, just after the last convolution layer, logistic function should be used\n",
    "     nn.Sigmoid()\n",
    "   )\n",
    "\n",
    "   self.softmax = nn.Softmax(dim=1)\n",
    "   # As required, here i need to use the MSE loss as the loss function\n",
    "   self.loss_fn = nn.MSELoss(reduction='sum') # will be divided by batch size\n",
    "\n",
    " def forward(self, in_data):\n",
    "   img_features = self.cnn_layers(in_data).view(in_data.size(0), 32,1,1) # in_data.size(0) == batch_size\n",
    "   logits = self.decoder(img_features)\n",
    "   return logits\n",
    "\n",
    " def loss(self, logits, labels):\n",
    "   #preds = self.softmax(logits) # size (batch_size, 10)\n",
    "   return self.loss_fn(logits, labels) / logits.size(0) # divided by batch_size\n",
    "\n",
    "\n",
    " #def top1_accuracy(self, logits, labels):\n",
    "   # get argmax of logits along dim=1 (this is equivalent to argmax of predicted probabilites)\n",
    "   #predicted_labels = torch.argmax(logits, dim=1, keepdim=False) # size (batch_size,)\n",
    "   #n_corrects = predicted_labels.eq(labels).sum(0) # sum up all the correct predictions\n",
    "   #return n_corrects / logits.size(0) * 100 # in percentage\n",
    "\n",
    " #def top3_accuracy(self, logits, labels):\n",
    "    #n_corrects = 0\n",
    "     # copy logits for implementation without chage original value\n",
    "    #logits_copy = logits.clone().detach()\n",
    "     # get argmax of logits along dim=1 (this is equivalent to argmax of predicted probabilites)\n",
    "    #max1 = torch.argmax(logits_copy, dim=1)\n",
    "    #n_corrects = n_corrects + max1.eq(labels).sum(0)\n",
    "    #for i in range(max1.shape[0]):\n",
    "    #    logits_copy[i][max1[i]] = -99999\n",
    "\n",
    "    #max2 = torch.argmax(logits_copy, dim=1)\n",
    "    #n_corrects = n_corrects + max2.eq(labels).sum(0)\n",
    "    #for i in range(max2.shape[0]):\n",
    "    #    logits_copy[i][max2[i]] = -99999\n",
    "\n",
    "    #max3 = torch.argmax(logits_copy, dim=1)\n",
    "    #n_corrects = n_corrects + max3.eq(labels).sum(0)\n",
    "    #for i in range(max3.shape[0]):\n",
    "    #    logits_copy[i][max3[i]] = -99999\n",
    "\n",
    "    #return n_corrects / logits.size(0) * 100 # in percentage\n",
    "    \n",
    "#####################################################################################################################\n",
    "# To train the CNN reconstructor, I wrote a new train function\n",
    "def train_reconstructor(model, loaders, optimizer, writer, n_epochs, ckpt_path, device='cpu'):\n",
    "  def run_epoch(train_or_eval):\n",
    "    epoch_loss = 0.\n",
    "\n",
    "    for i, batch in enumerate(loaders[train_or_eval], 1):\n",
    "      in_data, labels = batch\n",
    "      in_data, labels = in_data.to(device), labels.to(device)\n",
    "\n",
    "      if train_or_eval == 'train':\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "      logits = model(in_data)\n",
    "      \n",
    "      batch_loss = model.loss(logits, in_data)\n",
    "      \n",
    "     \n",
    "      epoch_loss += batch_loss\n",
    "        \n",
    "      if train_or_eval == 'train':\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "      if train_or_eval == 'eval':  \n",
    "        if len(logits.size()) == 2: # when it is flattened, reshape it\n",
    "          logits = logits.view(-1, 1, 28, 28)\n",
    "\n",
    "        img_grid = make_grid(logits.to('cpu'))\n",
    "        writer.add_image('%s/eval_input' % model.__class__.__name__, img_grid, epoch)\n",
    "\n",
    "    epoch_loss /= i\n",
    "    print('loss: %s' %epoch_loss)\n",
    "    print()\n",
    "\n",
    "    losses[train_or_eval] = epoch_loss\n",
    "\n",
    "    if writer is None:\n",
    "      print('epoch %d %s loss %.4f acc %.4f' % (epoch, train_or_eval, epoch_loss, epoch_acc))\n",
    "    elif train_or_eval == 'eval':\n",
    "      writer.add_scalars('%s_loss' % model.__class__.__name__,\n",
    "                         tag_scalar_dict={'train': losses['train'],\n",
    "                                          'eval': losses['eval']},\n",
    "                         global_step=epoch)\n",
    "\n",
    "\n",
    "     \n",
    "      \n",
    "    #  if len(logits.size()) == 2: # when it is flattened, reshape it\n",
    "    #    logits = logits.view(-1, 1, 28, 28)\n",
    "\n",
    "   #   img_grid = make_grid(logits.to('cpu'))\n",
    "   #   writer.add_image('%s/eval_input' % model.__class__.__name__, img_grid, epoch)\n",
    "\n",
    "  # main statements\n",
    "  losses = dict()\n",
    " \n",
    "\n",
    "  for epoch in range(1, n_epochs+1):\n",
    "    print('Epoch %s :' %epoch)\n",
    "    print('Train:')\n",
    "    run_epoch('train')\n",
    "    print('Eval:')\n",
    "    run_epoch('eval')\n",
    "\n",
    "    # For instructional purpose, show how to save checkpoints\n",
    "    if ckpt_path is not None:\n",
    "      torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'losses': losses\n",
    "      }, '%s/%d.pt' % (ckpt_path, epoch))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "########################################################################################################################   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. build your own CNN classifier with the given structure. DO NOT COPY OR USE ANY TRICK\n",
    "\n",
    "\n",
    "## Main function for implementing training\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam, SGD # just choose which to use\n",
    "\n",
    "\n",
    "def cnn_scratch_main(n_hidden,optim,learning_rate):\n",
    "  \n",
    "  gpu = -1 # default\n",
    "  #lr = args.lr\n",
    "  lr = learning_rate\n",
    "  #batch_size = args.batch\n",
    "  batch_size = 32 # default\n",
    "  #ckpt_path = args.ckpt\n",
    "  ckpt_path = './ckpt/cnn'\n",
    "  #n_epochs = args.epoch\n",
    "  n_epochs = 10 # default\n",
    "  #opt_str = args.optim\n",
    "  opt_str = optim\n",
    "\n",
    "  ckpt_path = '%s/%s' % (ckpt_path, opt_str)\n",
    "\n",
    "  if ckpt_path is not None:\n",
    "    if not(os.path.exists(ckpt_path)):\n",
    "      os.makedirs(ckpt_path)\n",
    "\n",
    "  if gpu == -1:\n",
    "    DEVICE = 'cpu'\n",
    "  elif torch.cuda.is_available():\n",
    "    DEVICE = gpu\n",
    "\n",
    "  #model = CnnClassifier(n_hidden).to(DEVICE)\n",
    "  # For the task 3.2.1, i need the Cnn_Scratch_Classifier model\n",
    "  model = Cnn_Scratch_Classifier(n_hidden)\n",
    "\n",
    "  train_dataset, test_dataset = get_datasets()\n",
    "\n",
    "  # Here I need to split the train_dataset in to train and eval with the ratio 4:1. I determine to set the last 20% as validation  dataset\n",
    "\n",
    "  n_samples = len(train_dataset)\n",
    "  eval_start = 0.8\n",
    "  split = int(np.floor(eval_start * n_samples))\n",
    "  indices = list(range(n_samples))\n",
    "  train_indices, validation_indices = indices[:split], indices[split:]\n",
    "\n",
    "  dataloaders = {\n",
    "    'train':  DataLoader(Subset(train_dataset, indices=train_indices),batch_size=batch_size, drop_last=False, shuffle=True),\n",
    "    'eval':   DataLoader(Subset(train_dataset, indices=validation_indices),batch_size=batch_size, drop_last=False, shuffle=True)\n",
    "  }\n",
    "\n",
    "  if opt_str == 'adam':\n",
    "    opt_class = Adam\n",
    "  elif opt_str == 'sgd':\n",
    "    opt_class = SGD\n",
    "\n",
    "  optimizer = opt_class(model.parameters(), lr=lr)\n",
    "  writer = SummaryWriter('./logs/cnn_scratch/%s_%s_%s' % (opt_str,n_hidden,lr))\n",
    "\n",
    "  train(model, dataloaders, optimizer, writer, n_epochs, ckpt_path, DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#if __name__=='__main__':\n",
    "\n",
    "  # Task 3.2.1 : Use my own CNN classifier\n",
    "  #cnn_scratch_main(32,'adam',0.001)\n",
    " # cnn_scratch_main(64,'adam',0.001)\n",
    " # cnn_scratch_main(32,'sgd',0.1)\n",
    " # cnn_scratch_main(64,'sgd',0.1)\n",
    " # cnn_scratch_main(32,'sgd',0.01)\n",
    " # cnn_scratch_main(64,'sgd',0.01)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 :\n",
      "Train:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-33a66a83dbaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn_scratch_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-50219420fdef>\u001b[0m in \u001b[0;36mcnn_scratch_main\u001b[0;34m(n_hidden, optim, learning_rate)\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./logs/cnn_scratch/%s_%s_%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mopt_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-4dcff0174120>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loaders, optimizer, writer, n_epochs, ckpt_path, device)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch %s :'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m     \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Eval:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-4dcff0174120>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(train_or_eval)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m       \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-4dcff0174120>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, in_data)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mimg_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# in_data.size(0) == batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnn_scratch_main(32,'adam',0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 :\n",
      "Train:\n",
      "top1 acc: 27.481003039513677     top3 acc: 49.55775075987842\n",
      "\n",
      "Eval:\n",
      "top1 acc: 48.79331306990881     top3 acc: 79.01519756838906\n",
      "\n",
      "Epoch 2 :\n",
      "Train:\n",
      "top1 acc: 56.651215805471125     top3 acc: 83.51215805471125\n",
      "\n",
      "Eval:\n",
      "top1 acc: 60.77811550151976     top3 acc: 87.04255319148936\n",
      "\n",
      "Epoch 3 :\n",
      "Train:\n",
      "top1 acc: 64.2530395136778     top3 acc: 88.2629179331307\n",
      "\n",
      "Eval:\n",
      "top1 acc: 66.40121580547113     top3 acc: 89.86626139817629\n",
      "\n",
      "Epoch 4 :\n",
      "Train:\n",
      "top1 acc: 68.40881458966565     top3 acc: 90.42629179331307\n",
      "\n",
      "Eval:\n",
      "top1 acc: 68.09118541033435     top3 acc: 90.60182370820668\n",
      "\n",
      "Epoch 5 :\n",
      "Train:\n",
      "top1 acc: 71.34042553191489     top3 acc: 91.79331306990882\n",
      "\n",
      "Eval:\n",
      "top1 acc: 70.1306990881459     top3 acc: 91.84802431610942\n",
      "\n",
      "Epoch 6 :\n",
      "Train:\n",
      "top1 acc: 73.15425531914893     top3 acc: 92.71048632218844\n",
      "\n",
      "Eval:\n",
      "top1 acc: 72.71428571428571     top3 acc: 93.0\n",
      "\n",
      "Epoch 7 :\n",
      "Train:\n",
      "top1 acc: 74.76367781155015     top3 acc: 93.31914893617021\n",
      "\n",
      "Eval:\n",
      "top1 acc: 73.04559270516717     top3 acc: 93.31306990881458\n",
      "\n",
      "Epoch 8 :\n",
      "Train:\n",
      "top1 acc: 75.80395136778115     top3 acc: 93.83662613981762\n",
      "\n",
      "Eval:\n",
      "top1 acc: 74.5015197568389     top3 acc: 93.77507598784194\n",
      "\n",
      "Epoch 9 :\n",
      "Train:\n",
      "top1 acc: 76.60258358662614     top3 acc: 94.30015197568389\n",
      "\n",
      "Eval:\n",
      "top1 acc: 75.96656534954407     top3 acc: 94.20060790273556\n",
      "\n",
      "Epoch 10 :\n",
      "Train:\n",
      "top1 acc: 77.28571428571429     top3 acc: 94.63373860182371\n",
      "\n",
      "Eval:\n",
      "top1 acc: 76.22492401215806     top3 acc: 94.19756838905775\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_scratch_main(64,'adam',0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 :\n",
      "Train:\n",
      "top1 acc: 2.069908814589666     top3 acc: 6.2803951367781155\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.024316109422492     top3 acc: 6.291793313069909\n",
      "\n",
      "Epoch 2 :\n",
      "Train:\n",
      "top1 acc: 2.033434650455927     top3 acc: 6.114741641337386\n",
      "\n",
      "Eval:\n",
      "top1 acc: 1.750759878419453     top3 acc: 5.7477203647416415\n",
      "\n",
      "Epoch 3 :\n",
      "Train:\n",
      "top1 acc: 2.040273556231003     top3 acc: 6.235562310030395\n",
      "\n",
      "Eval:\n",
      "top1 acc: 1.750759878419453     top3 acc: 5.890577507598784\n",
      "\n",
      "Epoch 4 :\n",
      "Train:\n",
      "top1 acc: 2.0972644376899696     top3 acc: 6.330547112462006\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.218844984802431\n",
      "\n",
      "Epoch 5 :\n",
      "Train:\n",
      "top1 acc: 1.987841945288754     top3 acc: 6.069148936170213\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.382978723404255\n",
      "\n",
      "Epoch 6 :\n",
      "Train:\n",
      "top1 acc: 2.1861702127659575     top3 acc: 6.28419452887538\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.069908814589666     top3 acc: 6.2097264437689965\n",
      "\n",
      "Epoch 7 :\n",
      "Train:\n",
      "top1 acc: 2.1268996960486324     top3 acc: 6.3373860182370825\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.13677811550152\n",
      "\n",
      "Epoch 8 :\n",
      "Train:\n",
      "top1 acc: 2.1246200607902734     top3 acc: 6.300911854103344\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.51063829787234\n",
      "\n",
      "Epoch 9 :\n",
      "Train:\n",
      "top1 acc: 2.1793313069908815     top3 acc: 6.37082066869301\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.25531914893617\n",
      "\n",
      "Epoch 10 :\n",
      "Train:\n",
      "top1 acc: 2.0653495440729484     top3 acc: 6.230243161094225\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 5.9361702127659575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_scratch_main(32,'sgd',0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 :\n",
      "Train:\n",
      "top1 acc: 2.1314589665653494     top3 acc: 6.198328267477204\n",
      "\n",
      "Eval:\n",
      "top1 acc: 1.9148936170212767     top3 acc: 5.735562310030395\n",
      "\n",
      "Epoch 2 :\n",
      "Train:\n",
      "top1 acc: 1.9582066869300911     top3 acc: 6.189969604863222\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.2006079027355625\n",
      "\n",
      "Epoch 3 :\n",
      "Train:\n",
      "top1 acc: 2.088145896656535     top3 acc: 6.354863221884498\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.270516717325228     top3 acc: 6.419452887537994\n",
      "\n",
      "Epoch 4 :\n",
      "Train:\n",
      "top1 acc: 2.2454407294832825     top3 acc: 6.370060790273556\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.270516717325228     top3 acc: 6.382978723404255\n",
      "\n",
      "Epoch 5 :\n",
      "Train:\n",
      "top1 acc: 2.188449848024316     top3 acc: 6.287993920972644\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.13677811550152\n",
      "\n",
      "Epoch 6 :\n",
      "Train:\n",
      "top1 acc: 2.154255319148936     top3 acc: 6.389817629179332\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.1580547112462005\n",
      "\n",
      "Epoch 7 :\n",
      "Train:\n",
      "top1 acc: 2.053951367781155     top3 acc: 6.122340425531915\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.13677811550152\n",
      "\n",
      "Epoch 8 :\n",
      "Train:\n",
      "top1 acc: 2.074468085106383     top3 acc: 6.4080547112462005\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.024316109422492     top3 acc: 5.592705167173253\n",
      "\n",
      "Epoch 9 :\n",
      "Train:\n",
      "top1 acc: 2.11322188449848     top3 acc: 6.217325227963526\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.428571428571429\n",
      "\n",
      "Epoch 10 :\n",
      "Train:\n",
      "top1 acc: 2.174772036474164     top3 acc: 6.250759878419453\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.0425531914893615     top3 acc: 6.428571428571429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_scratch_main(64,'sgd',0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 :\n",
      "Train:\n",
      "top1 acc: 2.012917933130699     top3 acc: 6.026595744680851\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.0516717325227964     top3 acc: 6.37386018237082\n",
      "\n",
      "Epoch 2 :\n",
      "Train:\n",
      "top1 acc: 2.167933130699088     top3 acc: 6.137537993920972\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.2948328267477205\n",
      "\n",
      "Epoch 3 :\n",
      "Train:\n",
      "top1 acc: 2.236322188449848     top3 acc: 6.446808510638298\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.237082066869301\n",
      "\n",
      "Epoch 4 :\n",
      "Train:\n",
      "top1 acc: 2.236322188449848     top3 acc: 6.429331306990881\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.13677811550152\n",
      "\n",
      "Epoch 5 :\n",
      "Train:\n",
      "top1 acc: 2.236322188449848     top3 acc: 6.466565349544073\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.13677811550152\n",
      "\n",
      "Epoch 6 :\n",
      "Train:\n",
      "top1 acc: 2.236322188449848     top3 acc: 6.48404255319149\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.139817629179332\n",
      "\n",
      "Epoch 7 :\n",
      "Train:\n",
      "top1 acc: 2.236322188449848     top3 acc: 6.47112462006079\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.13677811550152\n",
      "\n",
      "Epoch 8 :\n",
      "Train:\n",
      "top1 acc: 2.236322188449848     top3 acc: 6.51063829787234\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.13677811550152\n",
      "\n",
      "Epoch 9 :\n",
      "Train:\n",
      "top1 acc: 2.229483282674772     top3 acc: 6.446048632218845\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.13677811550152\n",
      "\n",
      "Epoch 10 :\n",
      "Train:\n",
      "top1 acc: 2.234042553191489     top3 acc: 6.561550151975684\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 5.890577507598784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_scratch_main(32,'sgd',0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 :\n",
      "Train:\n",
      "top1 acc: 2.0357142857142856     top3 acc: 6.018237082066869\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.4316109422492405\n",
      "\n",
      "Epoch 2 :\n",
      "Train:\n",
      "top1 acc: 2.197568389057751     top3 acc: 6.25531914893617\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.13677811550152\n",
      "\n",
      "Epoch 3 :\n",
      "Train:\n",
      "top1 acc: 2.229483282674772     top3 acc: 6.37917933130699\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.13677811550152\n",
      "\n",
      "Epoch 4 :\n",
      "Train:\n",
      "top1 acc: 2.202127659574468     top3 acc: 6.506079027355623\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.8996960486322187     top3 acc: 6.382978723404255\n",
      "\n",
      "Epoch 5 :\n",
      "Train:\n",
      "top1 acc: 2.1702127659574466     top3 acc: 6.39209726443769\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.13677811550152\n",
      "\n",
      "Epoch 6 :\n",
      "Train:\n",
      "top1 acc: 2.227203647416413     top3 acc: 6.326747720364741\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.264437689969605\n",
      "\n",
      "Epoch 7 :\n",
      "Train:\n",
      "top1 acc: 2.1633738601823707     top3 acc: 6.418693009118541\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.227963525835866\n",
      "\n",
      "Epoch 8 :\n",
      "Train:\n",
      "top1 acc: 2.202127659574468     top3 acc: 6.396656534954407\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 5.772036474164134\n",
      "\n",
      "Epoch 9 :\n",
      "Train:\n",
      "top1 acc: 2.2203647416413372     top3 acc: 6.4977203647416415\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.0820668693009114\n",
      "\n",
      "Epoch 10 :\n",
      "Train:\n",
      "top1 acc: 2.236322188449848     top3 acc: 6.504559270516717\n",
      "\n",
      "Eval:\n",
      "top1 acc: 2.115501519756839     top3 acc: 6.382978723404255\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_scratch_main(64,'sgd',0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the result I got, i would choose the adam 64 0.001 for the test\n",
    "# Before that i build a test function\n",
    "\n",
    "\n",
    "def cnn_scratch_test(n_hidden,optim,learning_rate):\n",
    "  gpu = -1 # default\n",
    "  \n",
    "  lr = learning_rate\n",
    "  \n",
    "  batch_size = 32 # default\n",
    "  #ckpt_path = args.ckpt\n",
    "  ckpt_path = './ckpt/cnn'\n",
    "  \n",
    "  n_epochs = 20 # default\n",
    " \n",
    "  opt_str = optim\n",
    "\n",
    "  ckpt_path = '%s/%s' % (ckpt_path, opt_str)\n",
    "\n",
    "  if ckpt_path is not None:\n",
    "    if not(os.path.exists(ckpt_path)):\n",
    "      os.makedirs(ckpt_path)\n",
    "\n",
    "  if gpu == -1:\n",
    "    DEVICE = 'cpu'\n",
    "  elif torch.cuda.is_available():\n",
    "    DEVICE = gpu\n",
    "\n",
    "  #model = CnnClassifier(n_hidden).to(DEVICE)\n",
    "  # For the task 3.2.1, i need the Cnn_Scratch_Classifier model\n",
    "  model = Cnn_Scratch_Classifier(n_hidden)\n",
    "\n",
    "  train_dataset, test_dataset = get_datasets()\n",
    "\n",
    "  # Here I need to split the train_dataset in to train and eval with the ratio 4:1. I determine to set the last 20% as validation  dataset\n",
    "\n",
    "  #n_samples = len(train_dataset)\n",
    "  #eval_start = 0.8\n",
    "  #split = int(np.floor(eval_start * n_samples))\n",
    "  #indices = list(range(n_samples))\n",
    "  #train_indices, validation_indices = indices[:split], indices[split:]\n",
    "\n",
    "  dataloaders = {\n",
    "    'train':  DataLoader(train_dataset,batch_size=batch_size, drop_last=False, shuffle=True),\n",
    "    'eval':   DataLoader(test_dataset,batch_size=batch_size, drop_last=False, shuffle=True)\n",
    "  }\n",
    "\n",
    "  if opt_str == 'adam':\n",
    "    opt_class = Adam\n",
    "  elif opt_str == 'sgd':\n",
    "    opt_class = SGD\n",
    "\n",
    "  optimizer = opt_class(model.parameters(), lr=lr)\n",
    "  writer = SummaryWriter('./logs/cnn_scratch/test_phase_%s_%s_%s' % (opt_str,n_hidden,lr))\n",
    "\n",
    "  train_for_test(model, dataloaders, optimizer, writer, n_epochs, ckpt_path, DEVICE)\n",
    "\n",
    "  ########################################\n",
    "  #the above part is the same as the train_main function\n",
    "  # Now i am gonna to use the trained model to do the test_dataset\n",
    "  loss =0.\n",
    "  acc_1= 0.\n",
    "  acc_3= 0. \n",
    "  dataloaders_test = DataLoader(test_dataset,batch_size=batch_size, drop_last=False, shuffle=True)  \n",
    "  \n",
    "  for i, batch in enumerate(dataloaders_test, 1):\n",
    "      in_data, labels = batch \n",
    "      logits = model(in_data)\n",
    "      batch_loss = model.loss(logits, labels)\n",
    "      batch_acc_1 = model.top1_accuracy(logits, labels)\n",
    "      batch_acc_3 = model.top3_accuracy(logits, labels)\n",
    "\n",
    "      loss +=  batch_loss\n",
    "      acc_1 += batch_acc_1\n",
    "      acc_3 += batch_acc_3\n",
    "    \n",
    "  loss /= i\n",
    "  acc_1 /= i\n",
    "  acc_3 /= i\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "  #in_data, labels = test_dataset\n",
    "  #logits = model(in_data)\n",
    "  #loss = model.loss(logits, labels)\n",
    "  #acc_1 = model.top1_accuracy(logits, labels)\n",
    "  #acc_3 = model.top3_accuracy(logits, labels)\n",
    "\n",
    " ###########\n",
    "  print('Test Phase:')\n",
    "  print('loss: %.4f acc1: %.4f acc3: %.4f' % (loss,acc_1,acc_3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 :\n",
      "Train:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2d0de5bdce62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn_scratch_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-3d76ac10f80e>\u001b[0m in \u001b[0;36mcnn_scratch_test\u001b[0;34m(n_hidden, optim, learning_rate)\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./logs/cnn_scratch/test_phase_%s_%s_%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mopt_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mtrain_for_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;31m########################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-4dcff0174120>\u001b[0m in \u001b[0;36mtrain_for_test\u001b[0;34m(model, loaders, optimizer, writer, n_epochs, ckpt_path, device)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch %s :'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-4dcff0174120>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(train_or_eval)\u001b[0m\n\u001b[1;32m    334\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtrain_or_eval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnn_scratch_test(64,'adam',0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_pretrained_main(n_hidden,optim,learning_rate):\n",
    "\n",
    "  gpu = -1 # default\n",
    "  #lr = args.lr\n",
    "  lr = learning_rate\n",
    "  #batch_size = args.batch\n",
    "  batch_size = 32 # default\n",
    "  #ckpt_path = args.ckpt\n",
    "  ckpt_path = './ckpt/cnn'\n",
    "  #n_epochs = args.epoch\n",
    "  n_epochs = 10# default\n",
    "  #opt_str = args.optim\n",
    "  opt_str = optim\n",
    "\n",
    "  ckpt_path = '%s/%s' % (ckpt_path, opt_str)\n",
    "\n",
    "  if ckpt_path is not None:\n",
    "    if not(os.path.exists(ckpt_path)):\n",
    "      os.makedirs(ckpt_path)\n",
    "\n",
    "  if gpu == -1:\n",
    "    DEVICE = 'cpu'\n",
    "  elif torch.cuda.is_available():\n",
    "    DEVICE = gpu\n",
    "\n",
    "  #model = CnnClassifier(n_hidden).to(DEVICE)\n",
    "  # For the task 3.2.1, i need the Cnn_Scratch_Classifier model\n",
    "  model = Cnn_Pretrained_Classifier(n_hidden)\n",
    "\n",
    "  train_dataset, test_dataset = get_datasets()\n",
    "\n",
    "  # Here I need to split the train_dataset in to train and eval with the ratio 4:1. I determine to set the last 20% as validation  dataset\n",
    "\n",
    "  n_samples = len(train_dataset)\n",
    "  eval_start = 0.8\n",
    "  split = int(np.floor(eval_start * n_samples))\n",
    "  indices = list(range(n_samples))\n",
    "  train_indices, validation_indices = indices[:split], indices[split:]\n",
    "\n",
    "  dataloaders = {\n",
    "    'train':  DataLoader(Subset(train_dataset, indices=train_indices),batch_size=batch_size, drop_last=False, shuffle=True),\n",
    "    'eval':   DataLoader(Subset(train_dataset, indices=validation_indices),batch_size=batch_size, drop_last=False, shuffle=True)\n",
    "  }\n",
    "\n",
    "  if opt_str == 'adam':\n",
    "    opt_class = Adam\n",
    "  elif opt_str == 'sgd':\n",
    "    opt_class = SGD\n",
    "\n",
    "  optimizer = opt_class(model.parameters(), lr=lr)\n",
    "  writer = SummaryWriter('./logs/cnn_pretrained/%s_%s_%s' % (opt_str,n_hidden,lr))\n",
    "\n",
    "  train(model, dataloaders, optimizer, writer, n_epochs, ckpt_path, DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 :\n",
      "Train:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-440b44ffd04a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn_pretrained_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-6c7b2e7a9bb2>\u001b[0m in \u001b[0;36mcnn_pretrained_main\u001b[0;34m(n_hidden, optim, learning_rate)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./logs/cnn_pretrained/%s_%s_%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mopt_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-4dcff0174120>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loaders, optimizer, writer, n_epochs, ckpt_path, device)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch %s :'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m     \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Eval:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-4dcff0174120>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(train_or_eval)\u001b[0m\n\u001b[1;32m    245\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtrain_or_eval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnn_pretrained_main(32,'adam',0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 :\n",
      "Train:\n",
      "loss: tensor(2.6514, grad_fn=<DivBackward0>)    top1 acc: 36.57446808510638     top3 acc: 61.2887537993921\n",
      "\n",
      "Eval:\n",
      "loss: tensor(1.7998, grad_fn=<DivBackward0>)    top1 acc: 52.525835866261396     top3 acc: 78.51671732522796\n",
      "\n",
      "Epoch 2 :\n",
      "Train:\n",
      "loss: tensor(1.5708, grad_fn=<DivBackward0>)    top1 acc: 56.287993920972646     top3 acc: 81.18465045592706\n",
      "\n",
      "Eval:\n",
      "loss: tensor(1.4274, grad_fn=<DivBackward0>)    top1 acc: 58.17325227963526     top3 acc: 83.40121580547113\n",
      "\n",
      "Epoch 3 :\n",
      "Train:\n",
      "loss: tensor(1.3497, grad_fn=<DivBackward0>)    top1 acc: 60.33206686930091     top3 acc: 84.11398176291793\n",
      "\n",
      "Eval:\n",
      "loss: tensor(1.3077, grad_fn=<DivBackward0>)    top1 acc: 60.27355623100304     top3 acc: 84.89057750759878\n",
      "\n",
      "Epoch 4 :\n",
      "Train:\n",
      "loss: tensor(1.2589, grad_fn=<DivBackward0>)    top1 acc: 62.34270516717325     top3 acc: 85.25607902735563\n",
      "\n",
      "Eval:\n",
      "loss: tensor(1.2352, grad_fn=<DivBackward0>)    top1 acc: 62.82066869300912     top3 acc: 86.03647416413374\n",
      "\n",
      "Epoch 5 :\n",
      "Train:\n",
      "loss: tensor(1.2088, grad_fn=<DivBackward0>)    top1 acc: 63.4612462006079     top3 acc: 86.12841945288754\n",
      "\n",
      "Eval:\n",
      "loss: tensor(1.1973, grad_fn=<DivBackward0>)    top1 acc: 63.735562310030396     top3 acc: 86.56838905775076\n",
      "\n",
      "Epoch 6 :\n",
      "Train:\n",
      "loss: tensor(1.1752, grad_fn=<DivBackward0>)    top1 acc: 64.49164133738601     top3 acc: 86.59270516717325\n",
      "\n",
      "Eval:\n",
      "loss: tensor(1.1642, grad_fn=<DivBackward0>)    top1 acc: 64.37082066869301     top3 acc: 87.05775075987842\n",
      "\n",
      "Epoch 7 :\n",
      "Train:\n",
      "loss: tensor(1.1529, grad_fn=<DivBackward0>)    top1 acc: 64.97948328267478     top3 acc: 87.00987841945289\n",
      "\n",
      "Eval:\n",
      "loss: tensor(1.1403, grad_fn=<DivBackward0>)    top1 acc: 65.209726443769     top3 acc: 87.51063829787235\n",
      "\n",
      "Epoch 8 :\n",
      "Train:\n",
      "loss: tensor(1.1330, grad_fn=<DivBackward0>)    top1 acc: 65.64817629179332     top3 acc: 87.20136778115501\n",
      "\n",
      "Eval:\n",
      "loss: tensor(1.1345, grad_fn=<DivBackward0>)    top1 acc: 65.61398176291793     top3 acc: 87.4984802431611\n",
      "\n",
      "Epoch 9 :\n",
      "Train:\n",
      "loss: tensor(1.1185, grad_fn=<DivBackward0>)    top1 acc: 65.919452887538     top3 acc: 87.42629179331307\n",
      "\n",
      "Eval:\n",
      "loss: tensor(1.1140, grad_fn=<DivBackward0>)    top1 acc: 65.85410334346504     top3 acc: 87.90881458966565\n",
      "\n",
      "Epoch 10 :\n",
      "Train:\n",
      "loss: tensor(1.1060, grad_fn=<DivBackward0>)    top1 acc: 66.20212765957447     top3 acc: 87.73252279635258\n",
      "\n",
      "Eval:\n",
      "loss: tensor(1.1126, grad_fn=<DivBackward0>)    top1 acc: 65.5258358662614     top3 acc: 87.72948328267478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_pretrained_main(64,'adam',0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 :\n",
      "Train:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-ea11cb7f9daf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn_pretrained_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sgd'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-6c7b2e7a9bb2>\u001b[0m in \u001b[0;36mcnn_pretrained_main\u001b[0;34m(n_hidden, optim, learning_rate)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./logs/cnn_pretrained/%s_%s_%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mopt_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-4dcff0174120>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loaders, optimizer, writer, n_epochs, ckpt_path, device)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch %s :'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m     \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Eval:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-4dcff0174120>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(train_or_eval)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m       \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-4dcff0174120>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, in_data)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0mimg_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# in_data.size(0) == batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnn_pretrained_main(32,'sgd',0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 :\n",
      "Train:\n",
      "loss: tensor(3.1381, grad_fn=<DivBackward0>)    top1 acc: 25.96352583586626     top3 acc: 47.72796352583587\n",
      "\n",
      "Eval:\n",
      "loss: tensor(2.1194, grad_fn=<DivBackward0>)    top1 acc: 42.650455927051674     top3 acc: 72.10030395136778\n",
      "\n",
      "Epoch 2 :\n",
      "Train:\n",
      "loss: tensor(1.6818, grad_fn=<DivBackward0>)    top1 acc: 52.952887537993924     top3 acc: 78.98176291793312\n",
      "\n",
      "Eval:\n",
      "loss: tensor(1.4444, grad_fn=<DivBackward0>)    top1 acc: 57.27051671732523     top3 acc: 82.74772036474164\n",
      "\n",
      "Epoch 3 :\n",
      "Train:\n",
      "loss: tensor(1.3517, grad_fn=<DivBackward0>)    top1 acc: 59.659574468085104     top3 acc: 83.77507598784194\n",
      "\n",
      "Eval:\n",
      "loss: tensor(1.2924, grad_fn=<DivBackward0>)    top1 acc: 60.99696048632219     top3 acc: 85.15197568389058\n",
      "\n",
      "Epoch 4 :\n",
      "Train:\n",
      "loss: tensor(1.2414, grad_fn=<DivBackward0>)    top1 acc: 62.53191489361702     top3 acc: 85.52127659574468\n",
      "\n",
      "Eval:\n",
      "loss: tensor(1.2033, grad_fn=<DivBackward0>)    top1 acc: 63.32522796352583     top3 acc: 86.46808510638297\n",
      "\n",
      "Epoch 5 :\n",
      "Train:\n",
      "loss: tensor(1.1865, grad_fn=<DivBackward0>)    top1 acc: 63.888297872340424     top3 acc: 86.46048632218844\n",
      "\n",
      "Eval:\n",
      "loss: tensor(1.1788, grad_fn=<DivBackward0>)    top1 acc: 63.51063829787234     top3 acc: 86.69604863221885\n",
      "\n",
      "Epoch 6 :\n",
      "Train:\n",
      "loss: tensor(1.1557, grad_fn=<DivBackward0>)    top1 acc: 64.67629179331307     top3 acc: 86.84574468085107\n",
      "\n",
      "Eval:\n",
      "loss: tensor(1.1539, grad_fn=<DivBackward0>)    top1 acc: 64.79939209726444     top3 acc: 87.1854103343465\n",
      "\n",
      "Epoch 7 :\n",
      "Train:\n",
      "loss: tensor(1.1322, grad_fn=<DivBackward0>)    top1 acc: 65.48100303951368     top3 acc: 87.34498480243161\n",
      "\n",
      "Eval:\n",
      "loss: tensor(1.1229, grad_fn=<DivBackward0>)    top1 acc: 65.73252279635258     top3 acc: 87.79635258358662\n",
      "\n",
      "Epoch 8 :\n",
      "Train:\n",
      "loss: tensor(1.1145, grad_fn=<DivBackward0>)    top1 acc: 65.75227963525836     top3 acc: 87.5015197568389\n",
      "\n",
      "Eval:\n",
      "loss: tensor(1.1058, grad_fn=<DivBackward0>)    top1 acc: 65.70212765957447     top3 acc: 87.75987841945289\n",
      "\n",
      "Epoch 9 :\n",
      "Train:\n",
      "loss: tensor(1.0990, grad_fn=<DivBackward0>)    top1 acc: 66.14893617021276     top3 acc: 87.84118541033435\n",
      "\n",
      "Eval:\n",
      "loss: tensor(1.1115, grad_fn=<DivBackward0>)    top1 acc: 65.58358662613982     top3 acc: 87.65045592705167\n",
      "\n",
      "Epoch 10 :\n",
      "Train:\n",
      "loss: tensor(1.0876, grad_fn=<DivBackward0>)    top1 acc: 66.72112462006079     top3 acc: 87.93617021276596\n",
      "\n",
      "Eval:\n",
      "loss: tensor(1.0850, grad_fn=<DivBackward0>)    top1 acc: 66.18844984802432     top3 acc: 88.26139817629179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_pretrained_main(64,'sgd',0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 :\n",
      "Train:\n",
      "loss: tensor(3.8424, grad_fn=<DivBackward0>)    top1 acc: 3.7066869300911853     top3 acc: 7.8465045592705165\n",
      "\n",
      "Eval:\n",
      "loss: tensor(3.8254, grad_fn=<DivBackward0>)    top1 acc: 5.079027355623101     top3 acc: 10.635258358662615\n",
      "\n",
      "Epoch 2 :\n",
      "Train:\n",
      "loss: tensor(3.7974, grad_fn=<DivBackward0>)    top1 acc: 7.170212765957447     top3 acc: 16.49544072948328\n",
      "\n",
      "Eval:\n",
      "loss: tensor(3.7677, grad_fn=<DivBackward0>)    top1 acc: 9.349544072948328     top3 acc: 21.35562310030395\n",
      "\n",
      "Epoch 3 :\n",
      "Train:\n",
      "loss: tensor(3.7213, grad_fn=<DivBackward0>)    top1 acc: 12.004559270516717     top3 acc: 27.475683890577507\n",
      "\n",
      "Eval:\n",
      "loss: tensor(3.6686, grad_fn=<DivBackward0>)    top1 acc: 14.46808510638298     top3 acc: 31.94224924012158\n",
      "\n",
      "Epoch 4 :\n",
      "Train:\n",
      "loss: tensor(3.5920, grad_fn=<DivBackward0>)    top1 acc: 17.404255319148938     top3 acc: 36.62537993920973\n",
      "\n",
      "Eval:\n",
      "loss: tensor(3.5075, grad_fn=<DivBackward0>)    top1 acc: 19.811550151975684     top3 acc: 41.08206686930091\n",
      "\n",
      "Epoch 5 :\n",
      "Train:\n",
      "loss: tensor(3.3995, grad_fn=<DivBackward0>)    top1 acc: 22.1451367781155     top3 acc: 45.340425531914896\n",
      "\n",
      "Eval:\n",
      "loss: tensor(3.2897, grad_fn=<DivBackward0>)    top1 acc: 23.267477203647417     top3 acc: 49.41337386018237\n",
      "\n",
      "Epoch 6 :\n",
      "Train:\n",
      "loss: tensor(3.1657, grad_fn=<DivBackward0>)    top1 acc: 27.420212765957448     top3 acc: 53.72872340425532\n",
      "\n",
      "Eval:\n",
      "loss: tensor(3.0467, grad_fn=<DivBackward0>)    top1 acc: 30.638297872340427     top3 acc: 57.796352583586625\n",
      "\n",
      "Epoch 7 :\n",
      "Train:\n",
      "loss: tensor(2.9216, grad_fn=<DivBackward0>)    top1 acc: 33.5790273556231     top3 acc: 60.505319148936174\n",
      "\n",
      "Eval:\n",
      "loss: tensor(2.8037, grad_fn=<DivBackward0>)    top1 acc: 35.91793313069909     top3 acc: 63.650455927051674\n",
      "\n",
      "Epoch 8 :\n",
      "Train:\n",
      "loss: tensor(2.6849, grad_fn=<DivBackward0>)    top1 acc: 38.254559270516715     top3 acc: 65.65349544072949\n",
      "\n",
      "Eval:\n",
      "loss: tensor(2.5746, grad_fn=<DivBackward0>)    top1 acc: 39.775075987841944     top3 acc: 67.76899696048632\n",
      "\n",
      "Epoch 9 :\n",
      "Train:\n",
      "loss: tensor(2.4668, grad_fn=<DivBackward0>)    top1 acc: 42.33738601823708     top3 acc: 69.27051671732522\n",
      "\n",
      "Eval:\n",
      "loss: tensor(2.3686, grad_fn=<DivBackward0>)    top1 acc: 42.10334346504559     top3 acc: 70.56534954407294\n",
      "\n",
      "Epoch 10 :\n",
      "Train:\n",
      "loss: tensor(2.2755, grad_fn=<DivBackward0>)    top1 acc: 45.13753799392097     top3 acc: 72.03267477203647\n",
      "\n",
      "Eval:\n",
      "loss: tensor(2.1908, grad_fn=<DivBackward0>)    top1 acc: 46.18541033434651     top3 acc: 73.33130699088146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_pretrained_main(32,'sgd',0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 :\n",
      "Train:\n",
      "loss: tensor(3.8346, grad_fn=<DivBackward0>)    top1 acc: 4.72644376899696     top3 acc: 10.544072948328267\n",
      "\n",
      "Eval:\n",
      "loss: tensor(3.8027, grad_fn=<DivBackward0>)    top1 acc: 7.267477203647417     top3 acc: 18.580547112462007\n",
      "\n",
      "Epoch 2 :\n",
      "Train:\n",
      "loss: tensor(3.7660, grad_fn=<DivBackward0>)    top1 acc: 10.24696048632219     top3 acc: 26.8031914893617\n",
      "\n",
      "Eval:\n",
      "loss: tensor(3.7229, grad_fn=<DivBackward0>)    top1 acc: 14.683890577507599     top3 acc: 37.02431610942249\n",
      "\n",
      "Epoch 3 :\n",
      "Train:\n",
      "loss: tensor(3.6594, grad_fn=<DivBackward0>)    top1 acc: 19.75987841945289     top3 acc: 44.55471124620061\n",
      "\n",
      "Eval:\n",
      "loss: tensor(3.5869, grad_fn=<DivBackward0>)    top1 acc: 21.835866261398177     top3 acc: 50.43465045592705\n",
      "\n",
      "Epoch 4 :\n",
      "Train:\n",
      "loss: tensor(3.4853, grad_fn=<DivBackward0>)    top1 acc: 26.553951367781156     top3 acc: 54.60182370820669\n",
      "\n",
      "Eval:\n",
      "loss: tensor(3.3759, grad_fn=<DivBackward0>)    top1 acc: 28.857142857142858     top3 acc: 58.483282674772035\n",
      "\n",
      "Epoch 5 :\n",
      "Train:\n",
      "loss: tensor(3.2394, grad_fn=<DivBackward0>)    top1 acc: 32.11702127659574     top3 acc: 60.17857142857143\n",
      "\n",
      "Eval:\n",
      "loss: tensor(3.1048, grad_fn=<DivBackward0>)    top1 acc: 34.90273556231003     top3 acc: 62.650455927051674\n",
      "\n",
      "Epoch 6 :\n",
      "Train:\n",
      "loss: tensor(2.9542, grad_fn=<DivBackward0>)    top1 acc: 37.97720364741642     top3 acc: 64.95136778115501\n",
      "\n",
      "Eval:\n",
      "loss: tensor(2.8160, grad_fn=<DivBackward0>)    top1 acc: 38.91793313069909     top3 acc: 67.61702127659575\n",
      "\n",
      "Epoch 7 :\n",
      "Train:\n",
      "loss: tensor(2.6738, grad_fn=<DivBackward0>)    top1 acc: 41.83966565349544     top3 acc: 69.0258358662614\n",
      "\n",
      "Eval:\n",
      "loss: tensor(2.5484, grad_fn=<DivBackward0>)    top1 acc: 43.285714285714285     top3 acc: 70.72340425531915\n",
      "\n",
      "Epoch 8 :\n",
      "Train:\n",
      "loss: tensor(2.4250, grad_fn=<DivBackward0>)    top1 acc: 45.204407294832826     top3 acc: 72.09802431610942\n",
      "\n",
      "Eval:\n",
      "loss: tensor(2.3169, grad_fn=<DivBackward0>)    top1 acc: 46.297872340425535     top3 acc: 73.66869300911854\n",
      "\n",
      "Epoch 9 :\n",
      "Train:\n",
      "loss: tensor(2.2151, grad_fn=<DivBackward0>)    top1 acc: 47.8563829787234     top3 acc: 74.43844984802432\n",
      "\n",
      "Eval:\n",
      "loss: tensor(2.1273, grad_fn=<DivBackward0>)    top1 acc: 48.87234042553192     top3 acc: 75.44984802431611\n",
      "\n",
      "Epoch 10 :\n",
      "Train:\n",
      "loss: tensor(2.0430, grad_fn=<DivBackward0>)    top1 acc: 50.20896656534954     top3 acc: 76.17173252279635\n",
      "\n",
      "Eval:\n",
      "loss: tensor(1.9709, grad_fn=<DivBackward0>)    top1 acc: 50.452887537993924     top3 acc: 77.5775075987842\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_pretrained_main(64,'sgd',0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_pretrained_test(n_hidden,optim,learning_rate):\n",
    "  gpu = -1 # default\n",
    "  \n",
    "  lr = learning_rate\n",
    "  \n",
    "  batch_size = 32 # default\n",
    "  #ckpt_path = args.ckpt\n",
    "  ckpt_path = './ckpt/cnn'\n",
    "  \n",
    "  n_epochs = 20 # default\n",
    " \n",
    "  opt_str = optim\n",
    "\n",
    "  ckpt_path = '%s/%s' % (ckpt_path, opt_str)\n",
    "\n",
    "  if ckpt_path is not None:\n",
    "    if not(os.path.exists(ckpt_path)):\n",
    "      os.makedirs(ckpt_path)\n",
    "\n",
    "  if gpu == -1:\n",
    "    DEVICE = 'cpu'\n",
    "  elif torch.cuda.is_available():\n",
    "    DEVICE = gpu\n",
    "\n",
    "  #model = CnnClassifier(n_hidden).to(DEVICE)\n",
    "  # For the task 3.2.1, i need the Cnn_Scratch_Classifier model\n",
    "  model = Cnn_Pretrained_Classifier(n_hidden)\n",
    "\n",
    "  train_dataset, test_dataset = get_datasets()\n",
    "\n",
    "  # Here I need to split the train_dataset in to train and eval with the ratio 4:1. I determine to set the last 20% as validation  dataset\n",
    "\n",
    "  #n_samples = len(train_dataset)\n",
    "  #eval_start = 0.8\n",
    "  #split = int(np.floor(eval_start * n_samples))\n",
    "  #indices = list(range(n_samples))\n",
    "  #train_indices, validation_indices = indices[:split], indices[split:]\n",
    "\n",
    "  dataloaders = {\n",
    "    'train':  DataLoader(train_dataset,batch_size=batch_size, drop_last=False, shuffle=True),\n",
    "    'eval':   DataLoader(test_dataset,batch_size=batch_size, drop_last=False, shuffle=True)\n",
    "  }\n",
    "\n",
    "  if opt_str == 'adam':\n",
    "    opt_class = Adam\n",
    "  elif opt_str == 'sgd':\n",
    "    opt_class = SGD\n",
    "\n",
    "  optimizer = opt_class(model.parameters(), lr=lr)\n",
    "  writer = SummaryWriter('./logs/cnn_pretrained/test_phase_%s_%s_%s' % (opt_str,n_hidden,lr))\n",
    "\n",
    "  train_for_test(model, dataloaders, optimizer, writer, n_epochs, ckpt_path, DEVICE)\n",
    "\n",
    "  ########################################\n",
    "  #the above part is the same as the train_main function\n",
    "  # Now i am gonna to use the trained model to do the test_dataset\n",
    "  loss =0.\n",
    "  acc_1= 0.\n",
    "  acc_3= 0. \n",
    "  dataloaders_test = DataLoader(test_dataset,batch_size=batch_size, drop_last=False, shuffle=True)  \n",
    "  \n",
    "  for i, batch in enumerate(dataloaders_test, 1):\n",
    "      in_data, labels = batch \n",
    "      logits = model(in_data)\n",
    "      batch_loss = model.loss(logits, labels)\n",
    "      batch_acc_1 = model.top1_accuracy(logits, labels)\n",
    "      batch_acc_3 = model.top3_accuracy(logits, labels)\n",
    "\n",
    "      loss += batch_loss\n",
    "      acc_1 += batch_acc_1\n",
    "      acc_3 += batch_acc_3\n",
    "    \n",
    "  loss /= i\n",
    "  acc_1 /= i\n",
    "  acc_3 /= i\n",
    "\n",
    " ###########\n",
    "  print('Test Phase:')\n",
    "  print('test loss: %.4f acc1: %.4f acc3: %.4f' % (loss,acc_1,acc_3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 :\n",
      "Train:\n",
      "top1 acc: 29.83161094224924     top3 acc: 53.06869300911854\n",
      "\n",
      "Test:\n",
      "top1 acc: 50.5752427184466     top3 acc: 77.81796116504854\n",
      "\n",
      "Epoch 2 :\n",
      "Train:\n",
      "top1 acc: 56.37629179331307     top3 acc: 81.87173252279635\n",
      "\n",
      "Test:\n",
      "top1 acc: 59.28883495145631     top3 acc: 83.18932038834951\n",
      "\n",
      "Epoch 3 :\n",
      "Train:\n",
      "top1 acc: 61.564741641337385     top3 acc: 85.22066869300912\n",
      "\n",
      "Test:\n",
      "top1 acc: 61.70631067961165     top3 acc: 85.28398058252426\n",
      "\n",
      "Epoch 4 :\n",
      "Train:\n",
      "top1 acc: 63.603039513677814     top3 acc: 86.44012158054711\n",
      "\n",
      "Test:\n",
      "top1 acc: 64.54611650485437     top3 acc: 86.62864077669903\n",
      "\n",
      "Epoch 5 :\n",
      "Train:\n",
      "top1 acc: 64.79452887537994     top3 acc: 87.08632218844984\n",
      "\n",
      "Test:\n",
      "top1 acc: 65.10436893203884     top3 acc: 87.3252427184466\n",
      "\n",
      "Epoch 6 :\n",
      "Train:\n",
      "top1 acc: 65.52158054711246     top3 acc: 87.54589665653495\n",
      "\n",
      "Test:\n",
      "top1 acc: 65.80097087378641     top3 acc: 87.17718446601941\n",
      "\n",
      "Epoch 7 :\n",
      "Train:\n",
      "top1 acc: 66.06079027355624     top3 acc: 87.80364741641337\n",
      "\n",
      "Test:\n",
      "top1 acc: 66.49271844660194     top3 acc: 87.84708737864078\n",
      "\n",
      "Epoch 8 :\n",
      "Train:\n",
      "top1 acc: 66.53738601823709     top3 acc: 88.24437689969605\n",
      "\n",
      "Test:\n",
      "top1 acc: 67.02912621359224     top3 acc: 88.34223300970874\n",
      "\n",
      "Epoch 9 :\n",
      "Train:\n",
      "top1 acc: 66.98905775075988     top3 acc: 88.50395136778116\n",
      "\n",
      "Test:\n",
      "top1 acc: 67.28155339805825     top3 acc: 88.51456310679612\n",
      "\n",
      "Epoch 10 :\n",
      "Train:\n",
      "top1 acc: 67.59756838905776     top3 acc: 88.79209726443769\n",
      "\n",
      "Test:\n",
      "top1 acc: 66.80339805825243     top3 acc: 88.37621359223301\n",
      "\n",
      "Epoch 11 :\n",
      "Train:\n",
      "top1 acc: 67.91793313069908     top3 acc: 89.06869300911855\n",
      "\n",
      "Test:\n",
      "top1 acc: 67.58980582524272     top3 acc: 88.75728155339806\n",
      "\n",
      "Epoch 12 :\n",
      "Train:\n",
      "top1 acc: 68.31975683890577     top3 acc: 89.33617021276596\n",
      "\n",
      "Test:\n",
      "top1 acc: 68.06796116504854     top3 acc: 89.0121359223301\n",
      "\n",
      "Epoch 13 :\n",
      "Train:\n",
      "top1 acc: 68.87537993920972     top3 acc: 89.4516717325228\n",
      "\n",
      "Test:\n",
      "top1 acc: 68.44174757281553     top3 acc: 89.25970873786407\n",
      "\n",
      "Epoch 14 :\n",
      "Train:\n",
      "top1 acc: 69.24559270516717     top3 acc: 89.82006079027356\n",
      "\n",
      "Test:\n",
      "top1 acc: 68.4004854368932     top3 acc: 89.50728155339806\n",
      "\n",
      "Epoch 15 :\n",
      "Train:\n",
      "top1 acc: 69.66565349544074     top3 acc: 89.97872340425532\n",
      "\n",
      "Test:\n",
      "top1 acc: 69.65291262135922     top3 acc: 89.6504854368932\n",
      "\n",
      "Epoch 16 :\n",
      "Train:\n",
      "top1 acc: 69.87051671732523     top3 acc: 90.24072948328268\n",
      "\n",
      "Test:\n",
      "top1 acc: 68.59708737864078     top3 acc: 89.72330097087378\n",
      "\n",
      "Epoch 17 :\n",
      "Train:\n",
      "top1 acc: 69.9969604863222     top3 acc: 90.30577507598784\n",
      "\n",
      "Test:\n",
      "top1 acc: 69.94174757281553     top3 acc: 89.75728155339806\n",
      "\n",
      "Epoch 18 :\n",
      "Train:\n",
      "top1 acc: 70.28024316109422     top3 acc: 90.48328267477204\n",
      "\n",
      "Test:\n",
      "top1 acc: 69.99757281553399     top3 acc: 90.09223300970874\n",
      "\n",
      "Epoch 19 :\n",
      "Train:\n",
      "top1 acc: 70.53434650455927     top3 acc: 90.72705167173253\n",
      "\n",
      "Test:\n",
      "top1 acc: 70.85194174757281     top3 acc: 90.10922330097087\n",
      "\n",
      "Epoch 20 :\n",
      "Train:\n",
      "top1 acc: 70.70881458966565     top3 acc: 90.86565349544072\n",
      "\n",
      "Test:\n",
      "top1 acc: 70.25242718446601     top3 acc: 90.36893203883496\n",
      "\n",
      "Test Phase:\n",
      "test loss: 0.9392 acc1: 70.3689 acc3: 90.3883\n"
     ]
    }
   ],
   "source": [
    "cnn_pretrained_test(64,'sgd',0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_reconstructor_main(optim,learning_rate):\n",
    "  \n",
    "  gpu = -1 # default\n",
    "  #lr = args.lr\n",
    "  lr = learning_rate\n",
    "  #batch_size = args.batch\n",
    "  batch_size = 32 # default\n",
    "  #ckpt_path = args.ckpt\n",
    "  ckpt_path = './ckpt/cnn'\n",
    "  #n_epochs = args.epoch\n",
    "  n_epochs = 10 # default\n",
    "  #opt_str = args.optim\n",
    "  opt_str = optim\n",
    "\n",
    "  ckpt_path = '%s/%s' % (ckpt_path, opt_str)\n",
    "\n",
    "  if ckpt_path is not None:\n",
    "    if not(os.path.exists(ckpt_path)):\n",
    "      os.makedirs(ckpt_path)\n",
    "\n",
    "  if gpu == -1:\n",
    "    DEVICE = 'cpu'\n",
    "  elif torch.cuda.is_available():\n",
    "    DEVICE = gpu\n",
    "\n",
    "  #model = CnnClassifier(n_hidden).to(DEVICE)\n",
    "  # For the task 3.2.1, i need the Cnn_Scratch_Classifier model\n",
    "  model = Cnn_Reconstructor()\n",
    "\n",
    "  train_dataset, test_dataset = get_datasets()\n",
    "\n",
    "  # Here I need to split the train_dataset in to train and eval with the ratio 4:1. I determine to set the last 20% as validation  dataset\n",
    "\n",
    "  n_samples = len(train_dataset)\n",
    "  eval_start = 0.8\n",
    "  split = int(np.floor(eval_start * n_samples))\n",
    "  indices = list(range(n_samples))\n",
    "  train_indices, validation_indices = indices[:split], indices[split:]\n",
    "\n",
    "  dataloaders = {\n",
    "    'train':  DataLoader(Subset(train_dataset, indices=train_indices),batch_size=batch_size, drop_last=False, shuffle=True),\n",
    "    'eval':   DataLoader(Subset(train_dataset, indices=validation_indices),batch_size=batch_size, drop_last=False, shuffle=True)\n",
    "  }\n",
    "\n",
    "  if opt_str == 'adam':\n",
    "    opt_class = Adam\n",
    "  elif opt_str == 'sgd':\n",
    "    opt_class = SGD\n",
    "\n",
    "  optimizer = opt_class(model.parameters(), lr=lr)\n",
    "  writer = SummaryWriter('./logs/cnn_reconstructor/%s_%s' % (opt_str,lr))\n",
    "\n",
    "  train_reconstructor(model, dataloaders, optimizer, writer, n_epochs, ckpt_path, DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 :\n",
      "Train:\n",
      "loss: tensor(56.0705, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(35.5156, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 2 :\n",
      "Train:\n",
      "loss: tensor(32.3901, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(30.1923, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 3 :\n",
      "Train:\n",
      "loss: tensor(29.0210, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(28.2592, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 4 :\n",
      "Train:\n",
      "loss: tensor(27.3596, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(26.9030, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 5 :\n",
      "Train:\n",
      "loss: tensor(26.3834, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(26.3067, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 6 :\n",
      "Train:\n",
      "loss: tensor(25.6910, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(25.4131, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 7 :\n",
      "Train:\n",
      "loss: tensor(25.0006, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(24.8564, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 8 :\n",
      "Train:\n",
      "loss: tensor(24.5487, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(24.6418, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 9 :\n",
      "Train:\n",
      "loss: tensor(24.1936, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(24.0925, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 10 :\n",
      "Train:\n",
      "loss: tensor(23.9187, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(23.8046, grad_fn=<DivBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_reconstructor_main('adam',0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 :\n",
      "Train:\n",
      "loss: tensor(65.7541, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(48.6323, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 2 :\n",
      "Train:\n",
      "loss: tensor(36.0014, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(30.8737, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 3 :\n",
      "Train:\n",
      "loss: tensor(27.3142, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(26.7220, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 4 :\n",
      "Train:\n",
      "loss: tensor(25.2783, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(23.5679, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 5 :\n",
      "Train:\n",
      "loss: tensor(24.1115, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(22.6146, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 6 :\n",
      "Train:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-abf9fa736ecd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn_reconstructor_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sgd'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-2efce740fac2>\u001b[0m in \u001b[0;36mcnn_reconstructor_main\u001b[0;34m(optim, learning_rate)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./logs/cnn_reconstructor/%s_%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mopt_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0mtrain_reconstructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-34c3d03f2424>\u001b[0m in \u001b[0;36mtrain_reconstructor\u001b[0;34m(model, loaders, optimizer, writer, n_epochs, ckpt_path, device)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch %s :'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m     \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Eval:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-34c3d03f2424>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(train_or_eval)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m       \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-34c3d03f2424>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, in_data)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m    \u001b[0mimg_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# in_data.size(0) == batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m    \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnn_reconstructor_main('sgd',0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 :\n",
      "Train:\n",
      "loss: tensor(111.1069, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(110.9498, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 2 :\n",
      "Train:\n",
      "loss: tensor(111.0610, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(110.9495, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 3 :\n",
      "Train:\n",
      "loss: tensor(111.0605, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(110.9490, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 4 :\n",
      "Train:\n",
      "loss: tensor(111.0601, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(110.9484, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 5 :\n",
      "Train:\n",
      "loss: tensor(111.0590, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(110.9469, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 6 :\n",
      "Train:\n",
      "loss: tensor(111.0565, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(110.9421, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 7 :\n",
      "Train:\n",
      "loss: tensor(108.7981, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(91.9122, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 8 :\n",
      "Train:\n",
      "loss: tensor(95.5920, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(87.6523, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 9 :\n",
      "Train:\n",
      "loss: tensor(94.9898, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(106.9808, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 10 :\n",
      "Train:\n",
      "loss: tensor(96.1447, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(94.6030, grad_fn=<DivBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_reconstructor_main('sgd',0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_reconstructor_test(optim,learning_rate):\n",
    "  gpu = -1 # default\n",
    "  \n",
    "  lr = learning_rate\n",
    "  \n",
    "  batch_size = 32 # default\n",
    "  #ckpt_path = args.ckpt\n",
    "  ckpt_path = './ckpt/cnn'\n",
    "  \n",
    "  n_epochs = 10 # default\n",
    " \n",
    "  opt_str = optim\n",
    "\n",
    "  ckpt_path = '%s/%s' % (ckpt_path, opt_str)\n",
    "\n",
    "  if ckpt_path is not None:\n",
    "    if not(os.path.exists(ckpt_path)):\n",
    "      os.makedirs(ckpt_path)\n",
    "\n",
    "  if gpu == -1:\n",
    "    DEVICE = 'cpu'\n",
    "  elif torch.cuda.is_available():\n",
    "    DEVICE = gpu\n",
    "\n",
    "  #model = CnnClassifier(n_hidden).to(DEVICE)\n",
    "  # For the task 3.2.1, i need the Cnn_Scratch_Classifier model\n",
    "  model = Cnn_Reconstructor()\n",
    "\n",
    "  train_dataset, test_dataset = get_datasets()\n",
    "\n",
    "\n",
    "  dataloaders = {\n",
    "    'train':  DataLoader(train_dataset,batch_size=batch_size, drop_last=False, shuffle=True),\n",
    "    'eval':   DataLoader(test_dataset,batch_size=batch_size, drop_last=False, shuffle=True)\n",
    "  }\n",
    "\n",
    "  if opt_str == 'adam':\n",
    "    opt_class = Adam\n",
    "  elif opt_str == 'sgd':\n",
    "    opt_class = SGD\n",
    "\n",
    "  optimizer = opt_class(model.parameters(), lr=lr)\n",
    "  writer = SummaryWriter('./logs/cnn_reconstructor/test_%s_%s' % (opt_str,lr))\n",
    "\n",
    "  train_reconstructor(model, dataloaders, optimizer, writer, n_epochs, ckpt_path, DEVICE)\n",
    "\n",
    "  ########################################\n",
    "  #the above part is the same as the train_main function\n",
    "  # Now i am gonna to use the trained model to do the test_dataset\n",
    "  loss =0.\n",
    "  acc_1= 0.\n",
    "  acc_3= 0. \n",
    "  dataloaders_test = DataLoader(test_dataset,batch_size=batch_size, drop_last=False, shuffle=True)  \n",
    "  \n",
    "  for i, batch in enumerate(dataloaders_test, 1):\n",
    "      in_data, labels = batch \n",
    "      logits = model(in_data)\n",
    "      batch_loss = model.loss(logits, in_data)\n",
    "      loss += batch_loss\n",
    "        \n",
    "      if len(logits.size()) == 2: # when it is flattened, reshape it\n",
    "          logits = logits.view(-1, 1, 28, 28)\n",
    "\n",
    "      img_grid = make_grid(logits.to('cpu'))\n",
    "\n",
    "      writer.add_image('%s/test_final_reconstruction' % model.__class__.__name__, img_grid, 10)\n",
    "\n",
    "\n",
    "    \n",
    "  loss /= i\n",
    " ###########\n",
    "  print('Test Phase:')\n",
    "  print('test loss: %.4f ' % loss)\n",
    "    \n",
    " # if len(logits.size()) == 2: # when it is flattened, reshape it\n",
    " #         logits = logits.view(-1, 1, 28, 28)\n",
    "\n",
    " # img_grid = make_grid(logits.to('cpu'))\n",
    "\n",
    " # writer.add_image('%s/test_reconstruction' % model.__class__.__name__, img_grid, 1)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 :\n",
      "Train:\n",
      "loss: tensor(60.4159, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(43.0084, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 2 :\n",
      "Train:\n",
      "loss: tensor(33.4852, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(27.2875, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 3 :\n",
      "Train:\n",
      "loss: tensor(25.6187, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(23.7592, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 4 :\n",
      "Train:\n",
      "loss: tensor(23.8127, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(22.3277, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 5 :\n",
      "Train:\n",
      "loss: tensor(23.0043, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(22.4310, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 6 :\n",
      "Train:\n",
      "loss: tensor(22.5741, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(22.4024, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 7 :\n",
      "Train:\n",
      "loss: tensor(22.2219, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(21.3158, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 8 :\n",
      "Train:\n",
      "loss: tensor(21.8545, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(21.1747, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 9 :\n",
      "Train:\n",
      "loss: tensor(21.6566, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(21.2691, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch 10 :\n",
      "Train:\n",
      "loss: tensor(21.4884, grad_fn=<DivBackward0>)\n",
      "\n",
      "Eval:\n",
      "loss: tensor(21.8991, grad_fn=<DivBackward0>)\n",
      "\n",
      "Test Phase:\n",
      "test loss: 21.8967 \n"
     ]
    }
   ],
   "source": [
    "cnn_reconstructor_test('sgd',0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
